{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"1\"></a>\n",
    "    <h3 id=\"#1\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">1. Introdução </h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A estratégia adotada para o desenvolvimento do modelo de classificação de reclamações envolveu uma abordagem integrada e iterativa, focada tanto no processamento de dados quanto na modelagem. Esta estratégia pode ser descrita em várias fases-chave, que são detalhadas a seguir:\n",
    "\n",
    "- **Pré-processamento de Dados Rigoroso**: A primeira fase da estratégia envolveu um pré-processamento de dados abrangente, visando limpar e normalizar os textos das reclamações. Operações como a remoção de URLs, tags HTML, pontuações, números, acentos, e a conversão de todos os caracteres para minúsculas foram cruciais para reduzir a complexidade dos dados. A lematização e a remoção de stopwords específicas do idioma Português ajudaram a focar no conteúdo semântico relevante, minimizando o ruído e as redundâncias nos dados.\n",
    "\n",
    "- **Transformação dos Textos em Estruturas Numéricas**: Utilizando técnicas de tokenização e padding, os textos foram convertidos em sequências numéricas de comprimento fixo, preparando-os para o processamento pela rede neural. A tokenização foi essencial para transformar o texto em uma forma que a rede neural pudesse interpretar, enquanto o padding garantiu que todas as sequências tivessem o mesmo tamanho, uma necessidade técnica para o treinamento do modelo.\n",
    "\n",
    "- **Arquitetura de Modelo Avançada**: A escolha de uma rede neural sequencial com camadas de Embedding e LSTM Bidirecional representou uma estratégia focada em capturar a natureza sequencial e os contextos bidirecionais do texto. A camada de Embedding proporcionou uma representação densa e significativa das palavras, enquanto as camadas LSTM Bidirecionais foram capazes de aprender dependências de longo prazo em ambas as direções do texto, aumentando a capacidade de compreensão do modelo.\n",
    "\n",
    "- **Técnicas de Regularização e Controle de Overfitting**: Para combater o overfitting, foram aplicadas técnicas de regularização L2 nas camadas LSTM, além do uso de dropout para adicionar aleatoriedade no processo de aprendizado, reduzindo a dependência do modelo em características específicas do conjunto de treinamento. O Early Stopping foi implementado para monitorar a acurácia de validação e interromper o treinamento assim que o modelo começasse a mostrar sinais de overfitting, garantindo que o modelo retido fosse aquele com o melhor desempenho geral sem ser excessivamente ajustado aos dados de treinamento.\n",
    "\n",
    "- **Avaliação e Iteração**: Após o treinamento inicial, o modelo foi rigorosamente avaliado usando métricas como precisão, recall, e F1-score para cada categoria de classificação. Essa avaliação permitiu identificar categorias onde o modelo poderia ser melhorado, guiando ajustes subsequentes no pré-processamento, na arquitetura do modelo, ou nas técnicas de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"2\"></a>\n",
    "    <h3 id=\"#2\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">2. Importar bibliotecas</h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from pt-core-news-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/uesleisutil/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import boto3\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import unicodedata\n",
    "from numpy import argmax\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dotenv import load_dotenv\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "! python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"3\"></a>\n",
    "    <h3 id=\"#3\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">3. Carregar Dataset, Parâmetros e Funções</h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load data from S3 due to An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key Id you provided does not exist in our records.. Attempting to load from local directory.\n",
      "Loaded data from local directory.\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables.\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "object_key = os.getenv('OBJECT_KEY')\n",
    "\n",
    "# Try to access AWS, if not, then load local data.\n",
    "try:\n",
    "    s3_client = boto3.client('s3', region_name='us-east-1', \n",
    "                         aws_access_key_id=aws_access_key_id, \n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "    csv_obj = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(csv_string),sep=';')\n",
    "    print(\"Loaded data from S3.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data from S3 due to {e}. Attempting to load from local directory.\")\n",
    "    local_path = '../data/tickets_reclamacoes_classificados.csv'\n",
    "    df = pd.read_csv(local_path,sep=';')\n",
    "    print(\"Loaded data from local directory.\")\n",
    "    \n",
    "# Portuguese data for Space.\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions.\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize the input string by converting to lowercase, removing punctuation,\n",
    "    digits, any special characters not included in the Portuguese alphabet, and\n",
    "    extra spaces. This function also applies accent removal.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input string to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    str: The normalized string.\n",
    "    \"\"\"\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^a-záéíóúàèìòùâêîôûãõäëïöüç\\s]', '', text)\n",
    "    text = ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn'))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize_and_remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to the input string and remove stopwords using the\n",
    "    spaCy library for the Portuguese language. Only non-stopword tokens are\n",
    "    lemmatized and concatenated into a single string.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input string to be lemmatized and from which stopwords will be removed.\n",
    "    \n",
    "    Returns:\n",
    "    str: The lemmatized string with stopwords removed.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            result.append(token.lemma_)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"4\"></a>\n",
    "    <h3 id=\"#4\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">4. Pré-processamento</h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalização dos dados\n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_reclamacao</th>\n",
       "      <th>data_abertura</th>\n",
       "      <th>categoria</th>\n",
       "      <th>descricao_reclamacao</th>\n",
       "      <th>descricao_reclamacao_norm</th>\n",
       "      <th>categoria_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3229299</td>\n",
       "      <td>2019-05-01T12:00:00-05:00</td>\n",
       "      <td>Hipotecas / Empréstimos</td>\n",
       "      <td>Bom dia, meu nome é xxxx xxxx e agradeço se vo...</td>\n",
       "      <td>dia nome xxxx xxxx agradeco voce puder ajudar ...</td>\n",
       "      <td>hipoteca emprestimos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3199379</td>\n",
       "      <td>2019-04-02T12:00:00-05:00</td>\n",
       "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
       "      <td>Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...</td>\n",
       "      <td>atualizei cartao xxxx xxxx informar agente atu...</td>\n",
       "      <td>Cartao credito cartao prepago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3233499</td>\n",
       "      <td>2019-05-06T12:00:00-05:00</td>\n",
       "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
       "      <td>O cartão Chase foi relatado em xx/xx/2019. No ...</td>\n",
       "      <td>cartao chase relatar entanto pedido fraudulent...</td>\n",
       "      <td>Cartao credito cartao prepago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3180294</td>\n",
       "      <td>2019-03-14T12:00:00-05:00</td>\n",
       "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
       "      <td>Em xx/xx/2018, enquanto tentava reservar um ti...</td>\n",
       "      <td>tentar reservar ticket xxxx xxxx deparei ofert...</td>\n",
       "      <td>Cartao credito cartao prepago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3224980</td>\n",
       "      <td>2019-04-27T12:00:00-05:00</td>\n",
       "      <td>Serviços de conta bancária</td>\n",
       "      <td>Meu neto me dê cheque por {$ 1600,00} Eu depos...</td>\n",
       "      <td>neto cheque depositar conta chase Fundo limpo ...</td>\n",
       "      <td>servico conta bancario</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_reclamacao              data_abertura  \\\n",
       "0        3229299  2019-05-01T12:00:00-05:00   \n",
       "1        3199379  2019-04-02T12:00:00-05:00   \n",
       "2        3233499  2019-05-06T12:00:00-05:00   \n",
       "3        3180294  2019-03-14T12:00:00-05:00   \n",
       "4        3224980  2019-04-27T12:00:00-05:00   \n",
       "\n",
       "                             categoria  \\\n",
       "0              Hipotecas / Empréstimos   \n",
       "1  Cartão de crédito / Cartão pré-pago   \n",
       "2  Cartão de crédito / Cartão pré-pago   \n",
       "3  Cartão de crédito / Cartão pré-pago   \n",
       "4           Serviços de conta bancária   \n",
       "\n",
       "                                descricao_reclamacao  \\\n",
       "0  Bom dia, meu nome é xxxx xxxx e agradeço se vo...   \n",
       "1  Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...   \n",
       "2  O cartão Chase foi relatado em xx/xx/2019. No ...   \n",
       "3  Em xx/xx/2018, enquanto tentava reservar um ti...   \n",
       "4  Meu neto me dê cheque por {$ 1600,00} Eu depos...   \n",
       "\n",
       "                           descricao_reclamacao_norm  \\\n",
       "0  dia nome xxxx xxxx agradeco voce puder ajudar ...   \n",
       "1  atualizei cartao xxxx xxxx informar agente atu...   \n",
       "2  cartao chase relatar entanto pedido fraudulent...   \n",
       "3  tentar reservar ticket xxxx xxxx deparei ofert...   \n",
       "4  neto cheque depositar conta chase Fundo limpo ...   \n",
       "\n",
       "                  categoria_norm  \n",
       "0           hipoteca emprestimos  \n",
       "1  Cartao credito cartao prepago  \n",
       "2  Cartao credito cartao prepago  \n",
       "3  Cartao credito cartao prepago  \n",
       "4         servico conta bancario  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applies normalizations.\n",
    "for col in ['descricao_reclamacao', 'categoria']:\n",
    "    df[f'{col}_norm'] = df[col].apply(normalize_text).apply(lemmatize_and_remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Separação entre teste e treino\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['descricao_reclamacao_norm']\n",
    "y = df['categoria_norm']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Stopwords e Vetorização\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Definição da Lista de Stopwords em Português**: Retorna uma lista de stopwords palavras em português.\n",
    "\n",
    "- **CountVectorizer com Restrição de Stopwords e Unigrama**: Converte uma coleção de documentos de texto em uma matriz de contagens de tokens vetorizados em Unigramas e instrui o vetorizador a ignorar as stopwords em portugês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_br_stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=pt_br_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Tokenização, limite de tamanho e *padding*\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização divide o texto em unidades menores (tokens), geralmente palavras ou termos. A tokenização é fundamental para a conversão de texto em uma forma que pode ser processada por algoritmos de aprendizado de máquina.\n",
    "\n",
    "Também limitamos o tamanho do vocabulário para ajudar a manter o modelo gerenciável, reduzindo a complexidade e o risco de *overfitting*.\n",
    "\n",
    "Como utilizamos um modelo de **Redes Neurais** para nossos dados e esse tipo de modelo geralmente requer entradas de tamanho fixo, aplicamos o *padding* para   garantir que todas as sequências de entrada tenham o mesmo comprimento, preenchendo sequências mais curtas com zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_encoded = to_categorical(y_train_encoded)\n",
    "y_test_encoded = to_categorical(y_test_encoded)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"5\"></a>\n",
    "    <h3 id=\"#5\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">5. Modelo e Validações</h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          2000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 200)               160800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 1005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2161805 (8.25 MB)\n",
      "Trainable params: 2161805 (8.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=False, dropout=0.5, recurrent_dropout=0.5, kernel_regularizer=l2(0.01))))\n",
    "model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "198/198 [==============================] - 99s 493ms/step - loss: 1.7936 - accuracy: 0.4977 - val_loss: 0.9930 - val_accuracy: 0.6264\n",
      "Epoch 2/8\n",
      "198/198 [==============================] - 115s 580ms/step - loss: 0.8940 - accuracy: 0.6858 - val_loss: 0.7539 - val_accuracy: 0.7713\n",
      "Epoch 3/8\n",
      "198/198 [==============================] - 109s 551ms/step - loss: 0.8565 - accuracy: 0.6945 - val_loss: 0.7845 - val_accuracy: 0.7001\n",
      "Epoch 4/8\n",
      "198/198 [==============================] - 111s 560ms/step - loss: 0.6423 - accuracy: 0.8121 - val_loss: 0.6622 - val_accuracy: 0.8086\n",
      "Epoch 5/8\n",
      "198/198 [==============================] - 108s 548ms/step - loss: 0.5519 - accuracy: 0.8549 - val_loss: 0.6127 - val_accuracy: 0.8219\n",
      "Epoch 6/8\n",
      "198/198 [==============================] - 110s 554ms/step - loss: 0.5017 - accuracy: 0.8661 - val_loss: 0.9543 - val_accuracy: 0.7105\n",
      "Epoch 7/8\n",
      "198/198 [==============================] - 107s 541ms/step - loss: 0.4591 - accuracy: 0.8834 - val_loss: 0.5846 - val_accuracy: 0.8387\n",
      "Epoch 8/8\n",
      "198/198 [==============================] - 102s 515ms/step - loss: 0.4167 - accuracy: 0.8958 - val_loss: 0.5724 - val_accuracy: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3296c48d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "model.fit(X_train_pad, y_train_encoded, epochs=8, batch_size=64, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 11s 64ms/step\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                                    0.88      0.70      0.78       549\n",
      "Cartao credito cartao prepago       0.86      0.87      0.87      1290\n",
      "         hipoteca emprestimos       0.86      0.87      0.86       922\n",
      "      roubo relatorio disputa       0.82      0.86      0.84      1204\n",
      "       servico conta bancario       0.87      0.89      0.88      1303\n",
      "\n",
      "                     accuracy                           0.85      5268\n",
      "                    macro avg       0.86      0.84      0.85      5268\n",
      "                 weighted avg       0.86      0.85      0.85      5268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_pad)\n",
    "y_pred_classes = argmax(y_pred, axis=1)\n",
    "y_test_classes = argmax(y_test_encoded, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: crimson; padding: 2px; border-radius: 15px; text-align: center;\">\n",
    "    <a class=\"anchor\" id=\"6\"></a>\n",
    "    <h3 id=\"#6\" style=\"color: white; font-size: 18px; font-family: 'Poppins', sans-serif; font-weight: bold;\">6. Conclusão</h3>  \n",
    "</div> \n",
    "\n",
    "[Voltar ao início](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de classificação desenvolvido demonstrou um desempenho geral robusto, com uma precisão (accuracy) de 85% no conjunto de testes. Isso indica uma capacidade significativa do modelo em identificar corretamente a categoria de reclamações entre as opções disponíveis: Cartão de Crédito/Cartão Pré-pago, Hipoteca/Empréstimos, Roubo/Relatório de Disputa e Serviço/Conta Bancária. As métricas individuais de precisão, recall e F1-score para cada categoria sugerem que o modelo é relativamente equilibrado em termos de desempenho entre as diferentes categorias, embora algumas variações sejam observáveis. Por exemplo, a categoria com a menor recall (0.70) demonstra que há espaço para melhorias na capacidade do modelo de identificar todos os casos relevantes para essa categoria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
